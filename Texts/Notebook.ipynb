{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ –ó–∞–¥–∞—á–∞\n",
    "\n",
    "–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫–∞–∫ **—Ç–æ–∫—Å–∏—á–Ω—ã–µ** –∏–ª–∏ **–Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã–µ**.  \n",
    "–í —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ ‚Äî –Ω–∞–±–æ—Ä —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "> üìå –¶–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞: **F1-score**  \n",
    "> üéØ –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: –∑–Ω–∞—á–µ–Ω–∏–µ **F1** –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å **–Ω–µ –º–µ–Ω—å—à–µ 0.75**\n",
    "\n",
    "**–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**  \n",
    "–§–∞–π–ª: `toxic_comments.csv`  \n",
    "- `text` ‚Äî —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è  \n",
    "- `toxic` ‚Äî —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ (1 ‚Äî —Ç–æ–∫—Å–∏—á–Ω—ã–π, 0 ‚Äî –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã–π)\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ –ü–ª–∞–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "–ë—É–¥—É—Ç –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã 3 –ø–æ–¥—Ö–æ–¥–∞:\n",
    "\n",
    "1. **Spacy (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è) + TF-IDF + LogisticRegression**\n",
    "2. **BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å) + LogisticRegression**\n",
    "3. **BERT (–ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∞)**\n",
    "\n",
    "> ‚öôÔ∏è –ú–æ–¥–µ–ª—å ‚Ññ3 –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –∞—Ä–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ:  \n",
    "> GPU: `1√óV100 32GB`, CPU: `20 vCPU`, RAM: `64 GB`, SSD: `400 GB`\n",
    "\n",
    "---\n",
    "\n",
    "### üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n",
    "\n",
    "#### –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "\n",
    "#### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n",
    "- 2.1 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö  \n",
    "- 2.2 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è TF-IDF  \n",
    "  - 2.2.1 Spacy  \n",
    "  - 2.2.2 NLTK  \n",
    "  - 2.2.3 –ü–∞–π–ø–ª–∞–π–Ω  \n",
    "- 2.3 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å)\n",
    "\n",
    "#### –û–±—É—á–µ–Ω–∏–µ\n",
    "- 3.1 TF-IDF  \n",
    "- 3.2 BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å)  \n",
    "  - 3.2.1 –ö–æ–¥ —ç–º–±–µ–¥–∏–Ω–≥–∞ –º–æ–¥–µ–ª—å—é BERT  \n",
    "- 3.3 BERT (–ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∞)  \n",
    "  - 3.3.1 –ö–æ–¥ —Ñ–∞–π–Ω—Ç—é–Ω–∞ –º–æ–¥–µ–ª–∏ BERT  \n",
    "  - 3.3.2 –ö–æ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "\n",
    "#### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥\n",
    "\n",
    "–í —Ö–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–∞ –±—ã–ª–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã —Ç—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞ –∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:\n",
    "\n",
    "- **Spacy + TF-IDF + LogisticRegression** ‚Äî –ø—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è, –Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å.\n",
    "- **BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å) + LogisticRegression** ‚Äî —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–æ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.\n",
    "- **BERT (–¥–æ–æ–±—É—á–µ–Ω–Ω—ã–π)** ‚Äî –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ç—Ä–µ–±—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ F1 = **0.82**, —á—Ç–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥ 0.75.\n",
    "\n",
    "–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT –æ–∫–∞–∑–∞–ª–æ—Å—å –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º, –æ–±–µ—Å–ø–µ—á–∏–≤ –∫–∞—á–µ—Å—Ç–≤–æ, –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –¥–ª—è –∑–∞–∫–∞–∑—á–∏–∫–∞. –û–¥–Ω–∞–∫–æ —Å—Ç–æ–∏—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –≤—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–æ–∫–æ–ª–æ 4.5 —á–∞—Å–æ–≤ –Ω–∞ –º–æ—â–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ).\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –æ—Å—Ç–∞—ë—Ç—Å—è –≤–∞–∂–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ø–æ–¥—Ö–æ–¥–∞, –∏ –≤ —Ç–µ–∫—É—â–µ–º —Å–ª—É—á–∞–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞—é—Ç –µ—ë –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏.\n",
    "\n",
    "---\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–ø—Ä–æ—â—ë–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–æ—Å—Ç—å –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç –±–∏–±–∏–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import torch \n",
    "import transformers\n",
    "import time\n",
    "import gdown\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score, \n",
    "    precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∂–∞–µ–º 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –∫–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:\n",
    "1. —Å—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç (df) –¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞ TF-IDF\n",
    "2. –¥–∞—Ç–∞—Å–µ—Ç —Å —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é BERT (bert_basic_features)\n",
    "3. –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –º–æ–¥–µ–ª—å—é BERT finetuned (bert_finetuned_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è BERT –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>logit_class_0</th>\n",
       "      <th>logit_class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He can go fuck himself sideways with a spiky s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.777386</td>\n",
       "      <td>1.081903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However: if what you mean is that you want to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607142</td>\n",
       "      <td>-1.885007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks for the welcome!\\nHello Cobaltbluetony....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607144</td>\n",
       "      <td>-1.885006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Too many citations \\n\\nDuncan, I trimmed some ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607143</td>\n",
       "      <td>-1.885007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No it is not reasonable to assume. In fact, vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607142</td>\n",
       "      <td>-1.885007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  true_label  \\\n",
       "0  He can go fuck himself sideways with a spiky s...           1   \n",
       "1  However: if what you mean is that you want to ...           0   \n",
       "2  Thanks for the welcome!\\nHello Cobaltbluetony....           0   \n",
       "3  Too many citations \\n\\nDuncan, I trimmed some ...           0   \n",
       "4  No it is not reasonable to assume. In fact, vi...           0   \n",
       "\n",
       "   predicted_label  logit_class_0  logit_class_1  \n",
       "0                1      -1.777386       1.081903  \n",
       "1                0       2.607142      -1.885007  \n",
       "2                0       2.607144      -1.885006  \n",
       "3                0       2.607143      -1.885007  \n",
       "4                0       2.607142      -1.885007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è BERT —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>logit_class_0</th>\n",
       "      <th>logit_class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"\\n Ok, an seems pretty clear by now what edit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607143</td>\n",
       "      <td>-1.885006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Proposal for standard infobox for History of [...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607144</td>\n",
       "      <td>-1.885006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\n Hi, Pompous Ass!  ;) \\n\\nI know you thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.776320</td>\n",
       "      <td>1.082370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am under attack! \\n\\nYou guys are censoring ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.728215</td>\n",
       "      <td>1.096051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why include Peter and Emerich?==\\n\\nPeter the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.607143</td>\n",
       "      <td>-1.885006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  true_label  \\\n",
       "0  \"\\n Ok, an seems pretty clear by now what edit...           0   \n",
       "1  Proposal for standard infobox for History of [...           0   \n",
       "2  \"\\n\\n Hi, Pompous Ass!  ;) \\n\\nI know you thin...           1   \n",
       "3  I am under attack! \\n\\nYou guys are censoring ...           0   \n",
       "4  Why include Peter and Emerich?==\\n\\nPeter the ...           0   \n",
       "\n",
       "   predicted_label  logit_class_0  logit_class_1  \n",
       "0                0       2.607143      -1.885006  \n",
       "1                0       2.607144      -1.885006  \n",
       "2                1      -1.776320       1.082370  \n",
       "3                1      -1.728215       1.096051  \n",
       "4                0       2.607143      -1.885006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1sOAFvqh25epbD2hygxm4YgXpIX3cM14-\n",
      "From (redirected): https://drive.google.com/uc?id=1sOAFvqh25epbD2hygxm4YgXpIX3cM14-&confirm=t&uuid=6d85bccf-f190-41e2-9f47-4cb1d06ea58f\n",
      "To: /home/jovyan/work/bert_features.npy\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 489M/489M [00:06<00:00, 75.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\n",
      "(159292, 768)\n"
     ]
    }
   ],
   "source": [
    "# 1. —Å—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "print(\"‚úÖ –°—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω:\")\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "# 2. –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–µ—Ä—Ç–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∞\n",
    "csv_url = \"https://drive.google.com/uc?export=download&id=1NUbZWztWcmzge784nic5y5I6vwg9aczi\"\n",
    "bert_finetuned_prediction_val = pd.read_csv(csv_url)\n",
    "print(\"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è BERT –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
    "display(bert_finetuned_prediction_val.head())\n",
    "\n",
    "\n",
    "# 3. –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–µ—Ä—Ç–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∞\n",
    "csv_url = \"https://drive.google.com/uc?export=download&id=18zaoWg5QeXOgAc7yA9Y8Lb3xDu4Sw4az\"\n",
    "bert_finetuned_prediction_test = pd.read_csv(csv_url)\n",
    "print(\"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è BERT —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
    "display(bert_finetuned_prediction_test.head())\n",
    "\n",
    "# 4. –ø—Ä–∏–Ω–∞–∫–∏ –ø–æ—Å–ª–µ —ç–º–±–µ–¥–∏–Ω–≥–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –±–µ—Ä—Ç\n",
    "npy_file_id = \"1sOAFvqh25epbD2hygxm4YgXpIX3cM14-\"\n",
    "npy_url = f\"https://drive.google.com/uc?id={npy_file_id}\"\n",
    "npy_filename = \"bert_features.npy\"\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —á–µ—Ä–µ–∑ gdown\n",
    "gdown.download(npy_url, npy_filename, quiet=False)\n",
    "\n",
    "bert_basic_features = np.load(npy_filename, allow_pickle=True)\n",
    "print(\"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —ç–º–±–µ–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\")\n",
    "print(bert_basic_features.shape)\n",
    "\n",
    "corpus = df['text']\n",
    "y = df['toxic']\n",
    "bert_corpus = pd.DataFrame(bert_basic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–∞—Ç–∞—Å–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "results = pd.DataFrame(columns=(\n",
    "    'Lemm method',\n",
    "    'F1 val',\n",
    "    'F1 test',\n",
    "    'Precision val',\n",
    "    'Precision test',\n",
    "    'Recall val',\n",
    "    'Recall test',\n",
    "    'Training time (s)')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus[:5]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 116.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Ä–µ–º—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ spacy: 0.04584813117980957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è / –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –±–∏–±–∏–ª–æ—Ç–µ–∫–æ—ã–π Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner', 'textcat', 'attribute_role'])\n",
    "\n",
    "def spacy_clean_batch(texts):\n",
    "    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤ tqdm –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=1000, n_process=1), total=len(texts), desc=\"Lemmatizing\"):\n",
    "        yield ' '.join(\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop        # –∏—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "            and not token.is_punct      # –∏—Å–∫–ª—é—á–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "            and not token.like_url      # –∏—Å–∫–ª—é—á–∞–µ–º —Å—Å—ã–ª–∫–∏\n",
    "            and not token.like_email    # –∏—Å–∫–ª—é—á–∞–µ–º email'—ã\n",
    "            and not token.is_space      # –∏—Å–∫–ª—é—á–∞–µ–º –ø—Ä–æ–±–µ–ª—ã\n",
    "            and not token.is_digit      # –∏—Å–∫–ª—é—á–∞–µ–º —á–∏—Å—Ç–æ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
    "            and token.is_alpha          # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
    "        )\n",
    "\n",
    "# –∫–æ—Ä–ø—É—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–æ–∫, –∞ –Ω–µ pandas Series\n",
    "spacy_start = time.time()\n",
    "lemmatized_corpus = list(spacy_clean_batch(corpus.tolist()))\n",
    "spacy_lemm_time = time.time() - spacy_start\n",
    "print(f'–í—Ä–µ–º—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ spacy: {spacy_lemm_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª csv\n",
    "lc_spacy_out = pd.DataFrame({'lemmatized_text': lemmatized_corpus})\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train / test\n",
    "X_trainval_tf_idf_spacy, X_test_tf_idf_spacy, y_trainval_tf_idf_spacy, y_test_tf_idf_spacy  = train_test_split(\n",
    "    lemmatized_corpus, y, test_size=0.10, stratify=y, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Ä–µ–º—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ nltk: 1.6695582866668701\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã\n",
    "# –°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Punkt ‚Äî –Ω—É–∂–µ–Ω –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞ (word_tokenize)\n",
    "nltk.download('punkt')\n",
    "# –°–∫–∞—á–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å WordNet ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# –°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å POS-—Ç–µ–≥–≥–µ—Ä–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∞—Å—Ç—å —Ä–µ—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (nltk.pos_tag)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# –°–∫–∞—á–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (stopwords.words('english'))\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "pattern = re.compile(r'\\b[a-zA-Z]{2,}\\b')\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è POS\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def fast_lemmatize(texts):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        tokens = pattern.findall(text.lower())  # —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞\n",
    "        tagged = pos_tag(tokens)                # POS-—Ç–µ–≥–∏\n",
    "        lemmas = [\n",
    "            lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "            for word, pos in tagged\n",
    "            if word not in stops\n",
    "        ]\n",
    "        results.append(' '.join(lemmas))\n",
    "    return results\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ corpus ‚Äî Series, –¥–µ–ª–∞–π—Ç–µ .astype(str))\n",
    "nltk_start = time.time()\n",
    "lemmatized_corpus_nltk = fast_lemmatize(corpus)\n",
    "nltk_lemm_time = time.time() - nltk_start\n",
    "print(f'–í—Ä–µ–º—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ nltk: {nltk_lemm_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª csv\n",
    "lc_nltk_out = pd.DataFrame({'lemmatized_text': lemmatized_corpus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edit username hardcore metallica f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>match background colour seemingly stuck thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man try edit war guy constantly remove rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>real suggestion improvement wonder section sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lemmatized_text\n",
       "0  explanation edit username hardcore metallica f...\n",
       "1  match background colour seemingly stuck thank ...\n",
       "2  hey man try edit war guy constantly remove rel...\n",
       "3  real suggestion improvement wonder section sta...\n",
       "4                      sir hero chance remember page"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_nltk_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train / test\n",
    "X_trainval_tf_idf_nltk, X_test_tf_idf_nltk, y_trainval_tf_idf_nltk, y_test_tf_idf_nltk  = train_test_split(\n",
    "    lemmatized_corpus_nltk, y, test_size=0.10, stratify=y, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ü–∞–π–ø–ª–∞–π–Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø–∞–π–ø–ª–∞–π–Ω (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —É—Ç–µ—á–µ–∫)\n",
    "pipeline_tf_idf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1))),\n",
    "    ('svd', TruncatedSVD(n_components=200, random_state=seed)), \n",
    "    ('clf', LogisticRegression(class_weight='balanced', penalty='l1', solver='saga', random_state=seed))\n",
    "])\n",
    "\n",
    "param_distributions_tf_idf = {\n",
    "    'clf': [LogisticRegression(class_weight='balanced', penalty='l1', solver='saga', random_state=seed)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143362, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train / test\n",
    "X_trainval_bert_basic, X_test_bert_basic, y_trainval_bert_basic, y_test_bert_basic = train_test_split(\n",
    "    bert_basic_features, y, test_size=0.10, stratify=y, random_state=seed\n",
    ")\n",
    "\n",
    "X_trainval_bert_basic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GridSearch\n",
    "param_distributions_bert_basic = {\n",
    "    'C': np.arange(.5, 1, .5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –æ–±—É—á–µ–Ω–∏–µ GridSearchCV\n",
    "def tf_idf_training(name, X_trainval, X_test, y_trainval, y_test):\n",
    "    global results\n",
    "    \n",
    "    start = time.time()\n",
    "    rs = GridSearchCV(\n",
    "        pipeline_tf_idf,\n",
    "        param_grid=param_distributions_tf_idf,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        refit='f1'\n",
    "    )\n",
    "    \n",
    "    rs.fit(X_trainval, y_trainval)\n",
    "    rs_search_time = time.time() - start\n",
    "\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "    best_idx = rs.best_index_\n",
    "    cv_results_df = pd.DataFrame(rs.cv_results_)\n",
    "\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "    preds = rs.predict(X_test)\n",
    "    \n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "    row = {\n",
    "        'Lemm method': name,\n",
    "        'F1 val': round(rs.best_score_, 2),\n",
    "        'Precision val': round(cv_results_df.loc[best_idx, 'mean_test_precision'], 2),\n",
    "        'Recall val': round(cv_results_df.loc[best_idx, 'mean_test_recall'], 2),\n",
    "        'F1 test': round(f1_score(y_test, preds), 2),\n",
    "        'Precision test': round(precision_score(y_test, preds), 2),\n",
    "        'Recall test': round(recall_score(y_test, preds), 2),\n",
    "        'Training time (s)': round(rs_search_time, 2)\n",
    "    }\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π—Å—É\n",
    "    results = pd.concat([results, pd.DataFrame([row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladimir\\anaconda3\\envs\\practicum\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\Vladimir\\anaconda3\\envs\\practicum\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "tf_idf_training(\n",
    "    'NLTK + TF-IDF + Logreg', \n",
    "    X_trainval_tf_idf_nltk, \n",
    "    X_test_tf_idf_nltk, \n",
    "    y_trainval_tf_idf_nltk, \n",
    "    y_test_tf_idf_nltk\n",
    ")\n",
    "\n",
    "tf_idf_training(\n",
    "    'SPACY + TF-IDF + Logreg', \n",
    "    X_trainval_tf_idf_spacy, \n",
    "    X_test_tf_idf_spacy, \n",
    "    y_trainval_tf_idf_spacy, \n",
    "    y_test_tf_idf_spacy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemm method</th>\n",
       "      <th>F1 val</th>\n",
       "      <th>F1 test</th>\n",
       "      <th>Precision val</th>\n",
       "      <th>Precision test</th>\n",
       "      <th>Recall val</th>\n",
       "      <th>Recall test</th>\n",
       "      <th>Training time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLTK + TF-IDF + Logreg</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>224.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACY + TF-IDF + Logreg</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>194.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Lemm method  F1 val  F1 test  Precision val  Precision test  \\\n",
       "0   NLTK + TF-IDF + Logreg    0.63     0.64           0.51            0.52   \n",
       "1  SPACY + TF-IDF + Logreg    0.64     0.66           0.52            0.54   \n",
       "\n",
       "   Recall val  Recall test  Training time (s)  \n",
       "0        0.84         0.85             224.33  \n",
       "1        0.83         0.85             194.52  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –æ–±—É—á–µ–Ω–∏–µ GridSearchCV\n",
    "start = time.time()\n",
    "\n",
    "bert_basic_rs = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', penalty='l1', solver='saga', random_state=seed),\n",
    "    param_grid=param_distributions_bert_basic,\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit='f1'\n",
    ")\n",
    "\n",
    "bert_basic_rs.fit(X_trainval_bert_basic, y_trainval_bert_basic)\n",
    "\n",
    "bert_basic_rs_search_time = time.time() - start\n",
    "\n",
    "preds_bert_basic = bert_basic_rs.predict(X_test_bert_basic)\n",
    "\n",
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "best_idx = bert_basic_rs.best_index_\n",
    "cv_results_df = pd.DataFrame(bert_basic_rs.cv_results_)\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "row = {\n",
    "    'Lemm method': 'BERT_embedding + Logreg',\n",
    "    'F1 val': round(bert_basic_rs.best_score_, 2),\n",
    "    'Precision val': round(cv_results_df.loc[best_idx, 'mean_test_precision'], 2),\n",
    "    'Recall val': round(cv_results_df.loc[best_idx, 'mean_test_recall'], 2),\n",
    "    'F1 test': round(f1_score(y_test_bert_basic, preds_bert_basic), 2),\n",
    "    'Precision test': round(precision_score(y_test_bert_basic, preds_bert_basic), 2),\n",
    "    'Recall test': round(recall_score(y_test_bert_basic, preds_bert_basic), 2),\n",
    "    'Training time (s)': round(bert_basic_rs_search_time, 2)\n",
    "}\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π—Å—É\n",
    "results = pd.concat([results, pd.DataFrame([row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemm method</th>\n",
       "      <th>F1 val</th>\n",
       "      <th>F1 test</th>\n",
       "      <th>Precision val</th>\n",
       "      <th>Precision test</th>\n",
       "      <th>Recall val</th>\n",
       "      <th>Recall test</th>\n",
       "      <th>Training time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLTK + TF-IDF + Logreg</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>224.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACY + TF-IDF + Logreg</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>194.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT_embedding + Logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>358.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Lemm method  F1 val  F1 test  Precision val  Precision test  \\\n",
       "0   NLTK + TF-IDF + Logreg    0.63     0.64           0.51            0.52   \n",
       "1  SPACY + TF-IDF + Logreg    0.64     0.66           0.52            0.54   \n",
       "2  BERT_embedding + Logreg    0.65     0.65           0.51            0.51   \n",
       "\n",
       "   Recall val  Recall test  Training time (s)  \n",
       "0        0.84         0.85             224.33  \n",
       "1        0.83         0.85             194.52  \n",
       "2        0.88         0.89             358.74  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ö–æ–¥ —ç–º–±–µ–¥–∏–Ω–≥–∞ –º–æ–¥–µ–ª—å—é –±–µ—Ä—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import transformers\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏–¥–æ–≤\n",
    "# SEED = 42\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# –ü–æ–¥–∫–ª—é—á–∞–µ–º GPU, –µ—Å–ª–∏ –µ—Å—Ç—å\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "# MODEL_NAME = \"bert-base-uncased\"\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = transformers.AutoModel.from_pretrained(MODEL_NAME)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "# df = pd.read_csv(\"toxic_comments.csv\")\n",
    "# texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–±—Ä–µ–∑–∫–æ–π –¥–æ 512\n",
    "# print(\"Tokenizing...\")\n",
    "# encoded = tokenizer.batch_encode_plus(\n",
    "#    texts,\n",
    "#    add_special_tokens=True,\n",
    "#    max_length=512,\n",
    "#    truncation=True,\n",
    "#    padding=\"longest\",  # padding –¥–æ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ –≤ –±–∞—Ç—á–µ\n",
    "#    return_attention_mask=True,\n",
    "#    return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "# input_ids = encoded[\"input_ids\"]\n",
    "# attention_mask = encoded[\"attention_mask\"]\n",
    "\n",
    "# –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ GPU\n",
    "# input_ids = input_ids.to(device)\n",
    "# attention_mask = attention_mask.to(device)\n",
    "\n",
    "# –≠–º–±–µ–¥–¥–∏–Ω–≥\n",
    "# batch_size = 32\n",
    "# embeddings = []\n",
    "\n",
    "# print(\"Generating embeddings...\")\n",
    "# with torch.no_grad():\n",
    "#    for i in tqdm(range(0, input_ids.size(0), batch_size)):\n",
    "#        input_batch = input_ids[i:i + batch_size]\n",
    "#        mask_batch = attention_mask[i:i + batch_size]\n",
    "\n",
    "#        outputs = model(input_batch, attention_mask=mask_batch)\n",
    "#        cls_embeddings = outputs.last_hidden_state[0][:, 0, :]  # [CLS] —Ç–æ–∫–µ–Ω\n",
    "#        embeddings.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª\n",
    "# features = np.concatenate(embeddings, axis=0)\n",
    "# np.save(\"bert_features.npy\", features)\n",
    "\n",
    "# print(f\"Done! Embeddings shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT (–ø–æ—Å–ª–µ —Ñ–∞–π–Ω—Ç—é–Ω–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–µ\n",
    "y_true_val_bert = bert_finetuned_prediction_val['true_label']\n",
    "y_pred_val_bert = bert_finetuned_prediction_val['predicted_label']\n",
    "\n",
    "y_true_test_bert = bert_finetuned_prediction_test['true_label']\n",
    "y_pred_test_bert = bert_finetuned_prediction_test['predicted_label']\n",
    "\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "row = {\n",
    "    'Lemm method': 'BERT_finetuning + BERT',\n",
    "    'F1 val': round(f1_score(y_true_val_bert, y_pred_val_bert), 2),\n",
    "    'Precision val': round(precision_score(y_true_val_bert, y_pred_val_bert), 2),\n",
    "    'Recall val': round(recall_score(y_true_val_bert, y_pred_val_bert), 2),\n",
    "    'F1 test': round(f1_score(y_true_test_bert, y_pred_test_bert), 2),\n",
    "    'Precision test': round(precision_score(y_true_test_bert, y_pred_test_bert), 2),\n",
    "    'Recall test': round(recall_score(y_true_test_bert, y_pred_test_bert), 2),\n",
    "    'Training time (s)': round(16200, 2)\n",
    "}\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π—Å—É\n",
    "results = pd.concat([results, pd.DataFrame([row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemm method</th>\n",
       "      <th>F1 val</th>\n",
       "      <th>F1 test</th>\n",
       "      <th>Precision val</th>\n",
       "      <th>Precision test</th>\n",
       "      <th>Recall val</th>\n",
       "      <th>Recall test</th>\n",
       "      <th>Training time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT_finetuning + BERT</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>16200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT_embedding + Logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>358.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACY + TF-IDF + Logreg</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>194.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLTK + TF-IDF + Logreg</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>224.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Lemm method  F1 val  F1 test  Precision val  Precision test  \\\n",
       "3   BERT_finetuning + BERT    0.83     0.82           0.86            0.86   \n",
       "2  BERT_embedding + Logreg    0.65     0.65           0.51            0.51   \n",
       "1  SPACY + TF-IDF + Logreg    0.64     0.66           0.52            0.54   \n",
       "0   NLTK + TF-IDF + Logreg    0.63     0.64           0.51            0.52   \n",
       "\n",
       "   Recall val  Recall test  Training time (s)  \n",
       "3        0.79         0.78           16200.00  \n",
       "2        0.88         0.89             358.74  \n",
       "1        0.83         0.85             194.52  \n",
       "0        0.84         0.85             224.33  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='F1 val', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–æ–≥–æ, –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –≥–æ–¥–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ BERT —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ç–æ—Ä–æ–π —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –ó–∞–∫–∞–∑—á–∏–∫–∞ –∫ –∑–Ω–∞—á–µ–Ω–∏—é –º–µ—Ç—Ä–∏–∫–∏ (F1 score) –Ω–∞ —É—Ä–æ–≤–Ω–µ –Ω–µ –º–µ–Ω–µ–µ 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ö–æ–¥ —Ñ–∞–π–Ω—Ç—é–Ω–∞ –º–æ–¥–µ–ª–∏ –±–µ—Ä—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import torch\n",
    "#import time\n",
    "#from datasets import Dataset\n",
    "#from transformers import (\n",
    "#    BertTokenizerFast,\n",
    "#    BertForSequenceClassification,\n",
    "#    TrainingArguments,\n",
    "#    Trainer\n",
    "#)\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "#start_total = time.time()\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "#start = time.time()\n",
    "#df = pd.read_csv(\"/toxic_comments.csv\")\n",
    "#df[\"label\"] = df[\"toxic\"].astype(int)\n",
    "#print(f\"[1] Data loading time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train / val / test\n",
    "#start = time.time()\n",
    "#df_trainval, df_test = train_test_split(\n",
    "#    df, test_size=0.10, stratify=df[\"label\"], random_state=42\n",
    "#)\n",
    "#df_train, df_val = train_test_split(\n",
    "#    df_trainval, test_size=0.111, stratify=df_trainval[\"label\"], random_state=42\n",
    "#)\n",
    "#print(f\"[2] Data splitting time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# 3. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "#start = time.time()\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "#print(f\"[3] Tokenizer loading time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "#def tokenize_function(examples):\n",
    "#    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# 4. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ Dataset –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "#start = time.time()\n",
    "#train_dataset = Dataset.from_pandas(df_train).map(tokenize_function, batched=True)\n",
    "#val_dataset = Dataset.from_pandas(df_val).map(tokenize_function, batched=True)\n",
    "#test_dataset = Dataset.from_pandas(df_test).map(tokenize_function, batched=True)\n",
    "\n",
    "#for ds in [train_dataset, val_dataset, test_dataset]:\n",
    "#    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "#print(f\"[4] Tokenization time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# 5. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "#start = time.time()\n",
    "#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "#print(f\"[5] Model loading time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# 6. –ú–µ—Ç—Ä–∏–∫–∏\n",
    "#def compute_metrics(eval_pred):\n",
    "#    logits, labels = eval_pred\n",
    "#    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "#    preds = logits.argmax(axis=1)\n",
    "#    return {\n",
    "#        \"accuracy\": accuracy_score(labels, preds),\n",
    "#        \"precision\": precision_score(labels, preds),\n",
    "#        \"recall\": recall_score(labels, preds),\n",
    "#        \"f1\": f1_score(labels, preds),\n",
    "#        \"roc_auc\": roc_auc_score(labels, probs),\n",
    "#    }\n",
    "\n",
    "# 7. –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "#training_args = TrainingArguments(\n",
    "#    output_dir=\"./bert_results\",\n",
    "#    evaluation_strategy=\"epoch\",\n",
    "#    save_strategy=\"epoch\",\n",
    "#    logging_strategy=\"epoch\",\n",
    "#    num_train_epochs=3,\n",
    "#    per_device_train_batch_size=16,\n",
    "#    per_device_eval_batch_size=32,\n",
    "#    learning_rate=2e-5,\n",
    "#    weight_decay=0.01,\n",
    "#    load_best_model_at_end=True,\n",
    "#    metric_for_best_model=\"f1\",\n",
    "#    fp16=True,\n",
    "#    report_to=\"none\",\n",
    "#)\n",
    "\n",
    "# 8. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
    "#start = time.time()\n",
    "#trainer = Trainer(\n",
    "#    model=model,\n",
    "#    args=training_args,\n",
    "#    train_dataset=train_dataset,\n",
    "#    eval_dataset=val_dataset,\n",
    "#    compute_metrics=compute_metrics,\n",
    "#)\n",
    "#trainer.train()\n",
    "#print(f\"[8] Training time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# 9. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–º —Ç–µ—Å—Ç–µ\n",
    "#start = time.time()\n",
    "#test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "#print(\"\\nFinal test metrics:\", test_results)\n",
    "#print(f\"[9] Final evaluation time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "# 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "#start = time.time()\n",
    "#model.save_pretrained(\"./bert_model_finetuned\")\n",
    "#tokenizer.save_pretrained(\"./bert_model_finetuned\")\n",
    "#print(f\"[10] Save time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "#print(f\"\\nTotal time: {time.time() - start_total:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ö–æ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#import torch\n",
    "#import transformers\n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–º\n",
    "#model_path = \"./–∞—Ä–µ–Ω–¥–∞gpu/bert_finetune/model_for_sending_to_hf/\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –º–æ–¥–µ–ª—å\n",
    "#tokenizer = transformers.BertTokenizer.from_pretrained(model_path)\n",
    "#model = transformers.BertForSequenceClassification.from_pretrained(model_path)\n",
    "#model.eval()\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "#df = pd.read_csv('./toxic_comments.csv')\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "#df_trainval_bert, df_test_bert = train_test_split(\n",
    "#    df, test_size=0.10, stratify=df[\"toxic\"], random_state=42\n",
    "#)\n",
    "\n",
    "#X_test_bert = df_test_bert['text'].tolist()\n",
    "#y_test_bert = df_test_bert['toxic'].tolist()\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "#inputs = tokenizer(\n",
    "#    X_test_bert,\n",
    "#    padding=True,\n",
    "#    truncation=True,\n",
    "#    max_length=512,\n",
    "#    return_tensors=\"pt\"\n",
    "#)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –±–∞—Ç—á–µ–π (—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –º–æ–∂–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ–¥ –ø–∞–º—è—Ç—å GPU)\n",
    "#batch_size = 32\n",
    "#dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
    "#dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "#all_logits = []\n",
    "#all_predictions = []\n",
    "#start_time = time.time()\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    for input_ids_batch, attention_mask_batch in dataloader:\n",
    "#        input_ids_batch = input_ids_batch.to(device)\n",
    "#        attention_mask_batch = attention_mask_batch.to(device)\n",
    "\n",
    "#        outputs = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)\n",
    "#        logits = outputs.logits\n",
    "\n",
    "#        probs = torch.softmax(logits, dim=1)\n",
    "#        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "#        all_logits.append(logits.cpu())\n",
    "#        all_predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "#elapsed_time = time.time() - start_time\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ª–æ–≥–∏–∏—Ç—ã –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä\n",
    "#all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "\n",
    "# –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏\n",
    "#results_df = pd.DataFrame({\n",
    "#    \"text\": X_test_bert,\n",
    "#    \"true_label\": y_test_bert,\n",
    "#    \"predicted_label\": all_predictions,\n",
    "#})\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –ª–æ–≥–∏—Ç–∞–º–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
    "#for i in range(all_logits.shape[1]):\n",
    "#    results_df[f\"logit_class_{i}\"] = all_logits[:, i]\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV\n",
    "#results_df.to_csv(\"bert_inference_results.csv\", index=False)\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏\n",
    "#f1_test_bert = f1_score(y_test_bert, all_predictions, average='binary')\n",
    "#precision_bert = precision_score(y_test_bert, all_predictions, average='binary')\n",
    "#recall_bert = recall_score(y_test_bert, all_predictions, average='binary')\n",
    "\n",
    "#print(f\"F1 Score: {f1_test_bert:.4f}\")\n",
    "#print(f\"Precision: {precision_bert:.4f}\")\n",
    "#print(f\"Recall: {recall_bert:.4f}\")\n",
    "#print(f\"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {elapsed_time:.4f} —Å–µ–∫—É–Ω–¥\")\n",
    "#print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ bert_inference_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏, —É–¥–∞—á–Ω—ã–º –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –æ–∫–∞–∑–∞–ª—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω - —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –º–æ–¥–µ–ª–∏ BERT. –¢—Ä–µ–±—É–µ–º–æ–µ –∑–∞–∫–∞–∑—á–∏–∫–æ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –Ω–µ –Ω–∏–∂–µ 0.75 –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –∏ —Ä–∞–≤–Ω–æ 0.82, –æ–¥–Ω–∞–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å —Ä–µ—Å—É—Ä—Å–æ–µ–º–∫–æ—Å—Ç—å –¥–∞–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—Ç—å 4.5 —á–∞—Å–∞ —Ñ–∞–π–Ω—Ç—é–Ω–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: gpu - 1xV100 32GB, CPU - 20 VCPU, RAM - 64–ì–±, SSD - 400–ì–±.\n",
    "\n",
    "–í—Å–µ –∂–µ —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –æ—â—É—â—É–µ–Ω–∏–µ, —á—Ç–æ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –º–æ–∂–Ω–æ –±—ã–ª–æ –Ω–∞–º–Ω–æ–≥–æ –ø—Ä–æ—â–µ... –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, —Å–∫–æ—Ä–µ–µ –º–∏–Ω—É—Å —á–µ–º –ø–ª—é—Å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ö–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–∞ –±—ã–ª–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã —Ç—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞ –∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:\n",
    "\n",
    "- **Spacy + TF-IDF + LogisticRegression** ‚Äî –ø—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è, –Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å.\n",
    "- **BERT (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å) + LogisticRegression** ‚Äî —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–æ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.\n",
    "- **BERT (–¥–æ–æ–±—É—á–µ–Ω–Ω—ã–π)** ‚Äî –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Ç—Ä–µ–±—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ F1 = **0.82**, —á—Ç–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥ 0.75.\n",
    "\n",
    "–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT –æ–∫–∞–∑–∞–ª–æ—Å—å –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º, –æ–±–µ—Å–ø–µ—á–∏–≤ –∫–∞—á–µ—Å—Ç–≤–æ, –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –¥–ª—è –∑–∞–∫–∞–∑—á–∏–∫–∞. –û–¥–Ω–∞–∫–æ —Å—Ç–æ–∏—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –≤—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–æ–∫–æ–ª–æ 4.5 —á–∞—Å–æ–≤ –Ω–∞ –º–æ—â–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ).\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –æ—Å—Ç–∞—ë—Ç—Å—è –≤–∞–∂–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ø–æ–¥—Ö–æ–¥–∞, –∏ –≤ —Ç–µ–∫—É—â–µ–º —Å–ª—É—á–∞–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞—é—Ç –µ—ë –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏.\n",
    "\n",
    "---\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–ø—Ä–æ—â—ë–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–æ—Å—Ç—å –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3,
    "start_time": "2025-07-28T07:14:35.011Z"
   },
   {
    "duration": 21809,
    "start_time": "2025-07-28T07:14:35.016Z"
   },
   {
    "duration": 0,
    "start_time": "2025-07-28T07:14:56.827Z"
   },
   {
    "duration": 3329,
    "start_time": "2025-07-31T07:00:09.521Z"
   },
   {
    "duration": 20007,
    "start_time": "2025-07-31T07:00:12.853Z"
   },
   {
    "duration": 49681,
    "start_time": "2025-07-31T07:01:09.479Z"
   },
   {
    "duration": 6,
    "start_time": "2025-07-31T07:02:03.005Z"
   },
   {
    "duration": 7,
    "start_time": "2025-07-31T07:02:32.964Z"
   },
   {
    "duration": 4,
    "start_time": "2025-07-31T07:02:44.203Z"
   },
   {
    "duration": 5,
    "start_time": "2025-07-31T07:03:11.374Z"
   },
   {
    "duration": 954,
    "start_time": "2025-07-31T07:04:05.612Z"
   },
   {
    "duration": 6,
    "start_time": "2025-07-31T07:04:10.783Z"
   },
   {
    "duration": 2220,
    "start_time": "2025-07-31T07:08:29.589Z"
   },
   {
    "duration": 370,
    "start_time": "2025-07-31T07:08:37.261Z"
   },
   {
    "duration": 7,
    "start_time": "2025-07-31T07:09:04.394Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

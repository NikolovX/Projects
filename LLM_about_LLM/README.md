# üß† LLM about LLM ‚Äî Fine-Tuning Atlas

_–ü–æ–ª–Ω–∞—è –∫–∞—Ä—Ç–∞ –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π ‚Äî –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ Fine-Tuning –¥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö PEFT, LoRA, QLoRA –∏ RAG._

> –¶–µ–ª—å: –¥–∞—Ç—å –∏–Ω–∂–µ–Ω–µ—Ä—É —Å–∏—Å—Ç–µ–º–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π ‚Äî –∫–æ–≥–¥–∞, –∑–∞—á–µ–º –∏ –∫–∞–∫–æ–π –º–µ—Ç–æ–¥ –≤—ã–±—Ä–∞—Ç—å –ø–æ–¥ –∑–∞–¥–∞—á—É, —Ä–µ—Å—É—Ä—Å—ã –∏ –¥–∞–Ω–Ω—ã–µ.

---

## üìç 1. –≠–≤–æ–ª—é—Ü–∏—è –ø–æ–¥—Ö–æ–¥–æ–≤

```
Full Fine-Tuning
     ‚Üì
Partial / Head-only Tuning
     ‚Üì
Parameter-Efficient Fine-Tuning (PEFT)
     ‚îú‚îÄ LoRA / QLoRA
     ‚îú‚îÄ Adapters
     ‚îú‚îÄ Prefix / Prompt / P-Tuning
     ‚îú‚îÄ BitFit
     ‚Üì
High-Level Tuning
     ‚îú‚îÄ SFT (Supervised Fine-Tuning)
     ‚îú‚îÄ RLHF (Reinforcement Learning from Human Feedback)
     ‚Üì
Non-Training Adaptation
     ‚îú‚îÄ RAG (Retrieval-Augmented Generation)
     ‚îú‚îÄ In-Context Learning / Prompt Engineering
```

---

## üß© 1.1. –î–∏–∞–≥—Ä–∞–º–º–∞ —ç–≤–æ–ª—é—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–≤ (Mermaid)

```mermaid
graph TD
    A[Full Fine-Tuning<br/>üîß –û–±—É—á–∞—é—Ç—Å—è –≤—Å–µ –≤–µ—Å–∞] --> B[Partial / Head-only<br/>üí° –í–µ—Ä—Ö–Ω–∏–µ —Å–ª–æ–∏]
    B --> C[Parameter-Efficient Fine-Tuning (PEFT)]
    C --> D1[LoRA / QLoRA<br/>‚ö° Low-Rank –∞–¥–∞–ø—Ç–∞—Ü–∏—è]
    C --> D2[Adapters<br/>üß± –í—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –±–ª–æ–∫–∏]
    C --> D3[Prefix / Prompt / P-Tuning<br/>üéØ –û–±—É—á–∞–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã]
    C --> D4[BitFit<br/>ü™∂ –¢–æ–ª—å–∫–æ bias]
    D1 --> E1[SFT<br/>üìò Supervised Instruction]
    D2 --> E1
    D3 --> E1
    D4 --> E1
    E1 --> E2[RLHF<br/>üß† Human Feedback Reinforcement]
    E2 --> F1[RAG<br/>üîç Retrieval-Augmented Generation]
    E2 --> F2[In-Context / Prompt Engineering<br/>üí¨ –ë–µ–∑ –æ–±—É—á–µ–Ω–∏—è]
    F1 --> G[Hybrid Systems<br/>üß© PEFT + RAG + RLHF]
```

---

## ‚öôÔ∏è 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –°—É—Ç—å | –û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è | –ü—Ä–∏–º–µ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ |
|------------|------|--------------------|--------------------|
| **Full Fine-Tuning** | –û–±—É—á–∞—é—Ç—Å—è –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ | –ù–æ–≤–∞—è –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å, –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ | ü§ó Transformers |
| **Partial Fine-Tuning** | –û–±—É—á–∞—é—Ç—Å—è –≤–µ—Ä—Ö–Ω–∏–µ —Å–ª–æ–∏ / –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä | –ü–æ–¥—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥ –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ | PyTorch, HF Trainer |
| **Adapter Tuning** | –í—Å—Ç–∞–≤–∫–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –æ–±—É—á–∞–µ–º—ã—Ö –±–ª–æ–∫–æ–≤ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ | –ú—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã | PEFT, AdapterHub |
| **LoRA / QLoRA** | –û–±—É—á–∞—é—Ç—Å—è low-rank –º–∞—Ç—Ä–∏—Ü—ã –≤ attention-—Å–ª–æ—è—Ö | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π PEFT –¥–ª—è LLM | PEFT + bitsandbytes |
| **BitFit** | –û–±—É—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ bias-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã | –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã | PEFT |
| **Prefix / Prompt / P-Tuning** | –û–±—É—á–∞–µ–º—ã–µ embedding-–ø—Ä–µ—Ñ–∏–∫—Å—ã –≤–º–µ—Å—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ | –ó–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, QA | PEFT |
| **SFT / Instruction Tuning** | –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö ‚Äú–≤–æ–ø—Ä–æ—Å‚Äì–æ—Ç–≤–µ—Ç‚Äù | –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã–µ LLM (ChatGPT-—Å—Ç–∏–ª—å) | TRL, PEFT |
| **RLHF** | –û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å —á–µ–ª–æ–≤–µ–∫–∞ | ChatGPT, Claude, Gemini | TRL (PPOTrainer) |
| **RAG** | –ë–µ–∑ –æ–±—É—á–µ–Ω–∏—è: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ retrieval-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ | Q&A, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã | LangChain, LlamaIndex |
| **In-Context / Prompt Engineering** | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä—è–º–æ –≤ prompt‚Äôe | –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è | OpenAI API, vLLM |

---

## üß≠ 4.1. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç-–∫–∞—Ä—Ç–∞ –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞

```mermaid
flowchart TD
    A[üöÄ –ù–∞—á–∞–ª–æ: —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å] --> B{–ï—Å—Ç—å –º–æ—â–Ω–∞—è GPU<br/>–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö?}
    B -- –î–∞ --> C[üîß Full Fine-Tuning<br/>–û–±—É—á–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã]
    B -- –ù–µ—Ç --> D{–•–æ—Ç–∏—Ç–µ –¥–æ–æ–±—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –º–æ–¥–µ–ª–∏?}
    D -- –î–∞ --> E[‚öôÔ∏è Partial Fine-Tuning<br/>–û–±—É—á–∞–µ–º –≤–µ—Ä—Ö–Ω–∏–µ —Å–ª–æ–∏ / –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä]
    D -- –ù–µ—Ç --> F{–ù—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É<br/>–ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö?}
    F -- –î–∞ --> G[üí° PEFT-–ø–æ–¥—Ö–æ–¥—ã<br/>(LoRA, QLoRA, Adapters, Prefix)]
    F -- –ù–µ—Ç --> H{–•–æ—Ç–∏—Ç–µ –¥–æ–æ–±—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å–º–µ—â–µ–Ω–∏—è (bias)?}
    H -- –î–∞ --> I[ü™∂ BitFit<br/>–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–ø–¥–µ–π—Ç –≤–µ—Å–æ–≤]
    H -- –ù–µ—Ç --> J{–ù—É–∂–µ–Ω –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç / –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å?}
    J -- –î–∞ --> K[üó£Ô∏è SFT / RLHF<br/>–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏]
    J -- –ù–µ—Ç --> L{–ù—É–∂–Ω–æ –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è<br/>–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏?}
    L -- –î–∞ --> M[üîç RAG (Retrieval-Augmented Generation)<br/>–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ + –∫–æ–Ω—Ç–µ–∫—Å—Ç]
    L -- –ù–µ—Ç --> N[üí¨ In-Context Learning / Prompt Engineering<br/>–∞–¥–∞–ø—Ç–∞—Ü–∏—è –±–µ–∑ fine-tuning]
    M --> O[üèÅ –ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è<br/>RAG + LoRA / RLHF]
    K --> O
    G --> O
```

---

## üß∞ 6. Fine-Tuning Stack ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

```mermaid
graph LR
    subgraph Core["üß† –ú–æ–¥–µ–ª—å –∏ —è–¥—Ä–æ"]
        A1[Transformers<br/>ü§ó Hugging Face]
        A2[PyTorch / TensorFlow]
        A3[Datasets]
    end
    subgraph PEFT["‚öôÔ∏è Parameter-Efficient Fine-Tuning"]
        B1[PEFT<br/>Adapters, LoRA, Prefix, BitFit]
        B2[bitsandbytes<br/>8-bit / 4-bit quantization]
        B3[Accelerate / Deepspeed]
    end
    subgraph Behavior["üß† –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ"]
        C1[TRL<br/>RLHF, PPOTrainer, DPO]
        C2[SFT Pipelines<br/>Instruction Tuning]
    end
    subgraph Retrieval["üîç –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π"]
        D1[LangChain<br/>Context Assembly]
        D2[LlamaIndex<br/>Retrieval Pipelines]
        D3[FAISS / Chroma / Milvus<br/>Vector DBs]
    end
    subgraph Inference["üöÄ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ –¥–µ–ø–ª–æ–π"]
        E1[vLLM / TGI<br/>Fast Inference Engines]
        E2[Gradio / Streamlit<br/>UI Frontends]
        E3[Optuna / Weights&Biases<br/>Tracking, Tuning]
    end
    A1 --> B1
    A1 --> B2
    B1 --> C1
    B1 --> C2
    C1 --> D1
    C2 --> D2
    D1 --> E1
    D2 --> E1
    E1 --> E2
    E1 --> E3
```

---

## üèÅ 9. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ ‚Äî *LLM about LLM*

> –¢—ã –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ–±—É—á–∞–µ—à—å –º–æ–¥–µ–ª–∏ ‚Äî  
> —Ç—ã —É—á–∏—à—å –∏—Ö **—É—á–∏—Ç—å—Å—è**.  
>  
> –ö–∞–∂–¥–∞—è —ç–ø–æ—Ö–∞ fine-tuning –¥–µ–ª–∞–µ—Ç —à–∞–≥ –æ—Ç ‚Äú–º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ –≤–µ—Å–æ–≤‚Äù  
> –∫ **–ø–æ–Ω–∏–º–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Ü–µ–ª–µ–π –∏ —Å–º—ã—Å–ª–∞**.  
>  
> –û—Ç –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è ‚Äî –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∏.  
> –û—Ç —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ‚Äî –∫ —Å–∏—Å—Ç–µ–º–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–ø–æ–º–∏–Ω–∞—é—Ç, –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è –∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—Ç.  
>  
> **Full Fine-Tuning** ‚Äî —ç—Ç–æ —Å–∏–ª–∞.  
> **LoRA –∏ QLoRA** ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.  
> **RLHF** ‚Äî —ç—Ç–æ —ç–º–ø–∞—Ç–∏—è.  
> **RAG** ‚Äî —ç—Ç–æ –ø–∞–º—è—Ç—å.  
>  
> –ê –∏—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ AI,  
> —ç—Ç–æ **–∏—Å–∫—É—Å—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è —Å–∞–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è**.  
>  
> üß† *LLM about LLM* ‚Äî —ç—Ç–æ –Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è.  
> –≠—Ç–æ –¥–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ –º—ã—à–ª–µ–Ω–∏—è –∏–Ω–∂–µ–Ω–µ—Ä–∞,  
> –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–¥.
